{
  "works": [
    {
      "name": "Devo",
      "short_description": [
        "Cloud-native company offering real-time security analytics, machine learning, and scalable solutions for enterprise data management."
      ],
      "thumbnail": "devo.png",
      "period_time": "March 2020 - March 2024",
      "full_description": [
        "<P>Devo is a leading cloud-native logging and security analytics company dedicated to providing real-time insights and advanced analytics for enterprise data. Their robust platform empowers organizations to efficiently detect, analyze, and respond to security threats by leveraging the power of machine learning and a highly scalable architecture. Devo's solutions are designed to handle large volumes of data, ensuring that businesses can maintain high levels of performance and security. Serving a diverse range of industries, including finance and telecommunications, Devo enhances cybersecurity measures and optimizes data management practices, helping enterprises stay ahead of evolving threats and regulatory requirements. With a commitment to innovation and excellence, Devo is at the forefront of transforming how businesses approach security and data analytics.</P>"
      ],
      "contribution": [
        "<P>At Devo, I played a crucial role in the data ingestion team, specifically focusing on the load balancer. Given the sensitivity of the data and its importance for diagnosing potential attacks, the service needed to be both reliable and fast. Under my expertise, the system was able to handle an impressive ingestion volume of approximately 60TB per day. Additionally, I significantly contributed to Devo's efforts in adapting to the necessary standards for integration with U.S. federal systems, achieving FedRAMP certification. Furthermore, I participated in the creation of a multi-region and multi-cloud backup system, ensuring data redundancy and reliability across diverse environments. My work ensured that Devo's platform met the highest levels of security and compliance, enhancing the company's capability to serve its clients effectively.</P>"
      ],
      "links": [
        {
          "tag": "Website",
          "url": "https://www.devo.com/es/",
          "text": "Devo Website"
        }
      ],
      "projects": [
        {
          "name": "Improve existing load balancer",
          "description": [
            "<P>I contributed significantly to enhancing the performance of Devo's load balancer by implementing advanced strategies that improved fault tolerance and optimized the monitoring of data ingestion services. I meticulously analyzed the system's requirements and identified key areas for improvement, ensuring that the load balancer could handle varying levels of traffic with greater efficiency. My efforts in improving the fault tolerance mechanisms helped the system recover swiftly from unexpected failures, maintaining high availability and reliability. Additionally, I created support for the RELP (Reliable Event Logging Protocol) from scratch, ensuring reliable and efficient event logging across the system.</P>",
            "<P>Furthermore, I developed methods to balance the load based on the client, which facilitated more compact data queries. By ensuring that data was not unnecessarily mixed across all nodes, I streamlined the data retrieval process, making it faster and more efficient. This client-specific load balancing approach not only improved query performance but also reduced the overhead on the system. My contributions led to a more robust and responsive load balancing system, significantly enhancing the overall performance and user experience at Devo.</P>"
          ],
          "links": [
            {
              "tag": "Documentation",
              "url": "https://docs.devo.com/space/latest/94653692/Event+load+balancers",
              "text": "Load balancer documentation"
            }
          ],
          "technologies": [
            "Node.js",
            "C",
            "C++",
            "FIPS",
            "FedRAMP",
            "SonarQube",
            "Snyk",
            "Owasp",
            "Jenkins",
            "Gitlab CI/CD",
            "Kubernetes",
            "GIT",
            "Jira",
            "TDD",
            "BDD"
          ]
        },
        {
          "name": "Create a multi-cloud backup system",
          "description": [
            "<P>I spearheaded the development of a cutting-edge multi-cloud backup system from scratch, a crucial advancement for Devo's data redundancy and disaster recovery capabilities. This system was designed to store client logs across multiple regions and cloud environments, ensuring maximum data protection and availability. One of the key features of this system was its ability to configure storage types on-the-fly, allowing for seamless adjustments to the cloud storage configurations based on real-time needs and criteria.</P>",
            "<P>I implemented sophisticated data transition criteria, which automated the movement of data between various storage classes based on usage patterns and retention policies. This feature ensured that data was always stored in the most cost-effective and efficient manner, without compromising accessibility or performance.</P>",
            "<P>In addition to this, my system included robust node recovery mechanisms, enabling quick restoration of service in the event of a node failure. This significantly reduced downtime and maintained the integrity and availability of critical data. To further enhance the system's efficiency, I developed advanced data compaction methods for data nearing its end-of-life, streamlining storage usage and reducing overhead.</P>",
            "<P>Finally, I integrated precise system cleanup schedules, ensuring that obsolete data was purged in a timely manner, maintaining optimal system performance and storage health. My contributions to this multi-cloud backup system not only fortified Devo's data management capabilities but also ensured compliance with stringent security and data retention regulations. This project underscored my expertise in creating resilient, scalable, and highly efficient data storage solutions, reinforcing Devo's commitment to excellence in security and data analytics.</P>"
          ],
          "links": [],
          "technologies": [
            "Java",
            "AWS",
            "GCP",
            "FIPS",
            "FedRAMP",
            "SonarQube",
            "Snyk",
            "Owasp",
            "Jenkins",
            "Gitlab CI/CD",
            "Kubernetes",
            "GIT",
            "Jira",
            "TDD",
            "BDD"
          ]
        },
        {
          "name": "Integrating Quality and Security Reviews into the CI/CD Pipeline",
          "description": [
            "<P>I played a pivotal role in the integration of comprehensive quality and security checks into Devo's CI/CD pipeline. This initiative was instrumental in elevating the development process to meet stringent industry standards, including NIST, FIPS, and FedRAMP.</P>",
            "<P><b>Jenkins Pipeline Integration</b></P>",
            "<P>Initially, I focused on incorporating SonarQube into the Jenkins pipeline for continuous code quality analysis. I configured SonarQube to perform static code analysis, which allowed the team to detect code smells, bugs, and potential vulnerabilities early in the development cycle. This integration ensured that only high-quality code was merged, significantly reducing the number of issues that reached production.</P>",
            "<P><b>Transition to GitLab CI/CD</b></P>",
            "<P>Following the success with Jenkins, I transitioned the pipeline to GitLab CI/CD. I leveraged GitLab's advanced features to streamline the process and enhance the automation of code deployments. This move facilitated better collaboration and efficiency across the development team.</P>",
            "<P><b>Security Vulnerability Scanning</b></P>",
            "<P>To address security vulnerabilities, I integrated Snyk and OWASP into the CI/CD pipeline. Snyk provided real-time scanning of open source dependencies, identifying and suggesting fixes for known vulnerabilities. Meanwhile, OWASP tools were utilized to perform comprehensive security assessments, focusing on identifying and mitigating risks associated with web applications.</P>",
            "<P><b>Achieving Compliance Standards</b></P>",
            "<P>My meticulous work ensured that the pipeline adhered to the rigorous standards required by NIST, FIPS, and FedRAMP. I established automated checks and balances within the CI/CD process, ensuring that every code change underwent thorough quality and security reviews before deployment. This not only improved the overall security posture of the applications but also streamlined the compliance verification process, making it easier for Devo to demonstrate adherence to these critical standards.</P>",
            "<P><b>Outcome</b></P>",
            "<P>My contributions significantly enhanced the robustness of Devo's CI/CD pipeline, ensuring that code quality and security were maintained at the highest levels. The integration of SonarQube, Snyk, and OWASP tools into the pipeline resulted in a more secure and efficient development process, aligning with the companyâ€™s commitment to innovation and excellence. This project was a crucial step in maintaining Devo's position at the forefront of cybersecurity and data analytics, providing clients with reliable and secure solutions.</P>"
          ],
          "links": [],
          "technologies": [
            "Gitlab CI/CD",
            "Jenkins",
            "NIST",
            "FIPS",
            "FedRAMP",
            "SonarQube",
            "Snyk",
            "Owasp",
            "Kubernetes",
            "Jira"
          ]
        },
        {
          "name": "Creation of an Alert System for Monitoring with Prometheus",
          "description": [
            "<P>I was instrumental in the development of a comprehensive alert system designed to monitor performance and critical situations in production environments using Prometheus. Recognizing the need for real-time monitoring and prompt response mechanisms, I architected a solution that provided detailed insights into system performance and detected potential issues such as fraud, attacks, and other anomalies.</P>",
            "<P>To achieve this, I integrated Prometheus into Devo's infrastructure, setting up robust metrics collection and alerting mechanisms. I defined key performance indicators (KPIs) and thresholds that were crucial for maintaining system health and stability. By leveraging Prometheus' powerful querying capabilities, I established a series of alert rules that could identify abnormal patterns in data ingestion, processing times, and system resource usage.</P>",
            "<P>In addition to setting up the alert system, I developed a thorough troubleshooting guide to assist the operations team in diagnosing and resolving issues swiftly. This guide included step-by-step procedures for identifying the root cause of alerts, recommended actions, and escalation protocols. The guide ensured that team members had a clear understanding of how to handle different types of alerts, from minor performance degradations to critical system failures.</P>",
            "<P>Furthermore, I implemented automated early response actions to mitigate potential issues before they escalated. These automated actions included restarting services, redistributing load, and scaling resources dynamically based on the nature and severity of the alerts. By automating these initial responses, I significantly reduced the mean time to recovery (MTTR) and minimized the impact of issues on the end-users.</P>",
            "<P>To enhance the overall effectiveness of the monitoring system, I also designed protocols for handling critical system situations. These protocols outlined the roles and responsibilities of team members, communication strategies, and coordination efforts required during major incidents. This structured approach ensured a swift and coordinated response, reducing downtime and maintaining service reliability.</P>",
            "<P>My contributions to the creation of the alert system not only improved the monitoring capabilities of Devo's platform but also bolstered its security posture by enabling the early detection of fraud and attacks. My work ensured that the system could maintain high performance and resilience, providing Devo's clients with a reliable and secure data analytics platform.</P>"
          ],
          "links": [],
          "technologies": [
            "Prometheus",
            "Grafana",
            "Kubernetes",
            "Docker",
            "Elastic Stack",
            "Kibana",
            "Python",
            "Bash",
            "Ansible",
            "GIT"
          ]
        }
      ],
      "images": [
        "devo1.png",
        "devo2.png"
      ]
    },
    {
      "name": "European Commission",
      "short_description": [
        "The European Commission is the executive branch of the European Union, responsible for proposing legislation, implementing decisions, and managing day-to-day affairs."
      ],
      "thumbnail": "European_Commission_01.png",
      "period_time": "June 2017 - January 2020",
      "full_description": [
        "<P>The European Commission is the executive branch of the European Union, responsible for proposing legislation, implementing decisions, upholding the EU treaties, and managing the day-to-day business of the EU. It operates as a cabinet government, with 27 members of the Commission (one from each member state) led by a President. The Commission's main roles include developing strategies and policies, enforcing EU laws, and negotiating international agreements on behalf of the EU.</P>",
        "<P>Within the European Commission, the Directorate-General for Competition (DG COMP) plays a critical role. DG COMP is responsible for establishing and implementing competition policy to ensure fair competition within the EU internal market. This includes enforcing antitrust laws, overseeing mergers and acquisitions, and preventing monopolistic practices and state aid that could distort competition. DG COMP works to protect consumers and businesses by promoting innovation, ensuring a level playing field, and enhancing economic efficiency.</P>",
        "<P>I worked at DG COMP, contributing to the Commission's efforts to maintain competitive markets and foster economic growth across the European Union.</P>"
      ],
      "contribution": [
        "<P>At the European Commission, I worked as a software architect on three projects, with a wide range of responsibilities that showcased my expertise and leadership in software development. My key responsibilities included:</P>",
        "<ul>",
        "<li><b>Management of Non-Functional Requirements and Project Definition</b>: Using Enterprise Architect, I was responsible for identifying and managing non-functional requirements, ensuring that each project was clearly defined and aligned with organizational goals.</li>",
        "<li><b>Task Prioritization</b>: Utilizing Jira, I prioritized tasks effectively to ensure that the most critical elements were addressed in a timely manner.</li>",
        "<li><b>Management of Task Dependencies</b>: I used Jira to manage dependencies between tasks, ensuring smooth progress and minimizing delays.</li>",
        "<li><b>Technology Selection</b>: I was instrumental in selecting the appropriate technologies for each project, ensuring that they were well-suited to meet the projectâ€™s needs.</li>",
        "<li><b>Team Member Profile Definition</b>: I defined the profiles and roles of team members, ensuring that the team had the necessary skills and expertise to succeed.</li>",
        "<li><b>Inter-Team Collaboration</b>: I facilitated collaboration between different teams whose projects needed to be integrated, ensuring seamless integration and cooperation.</li>",
        "<li><b>Quality Control and Testing</b>: I defined quality controls and testing protocols, including unit tests using technologies such as JUnit or Google Test/Valgrind, depending on the programming language.</li>",
        "<li><b>CI/CD Systems Implementation</b>: I defined and implemented Continuous Integration and Continuous Deployment systems using tools like Bamboo and SonarQube.</li>",
        "<li><b>Software Development Methodologies</b>: I applied Test-Driven Development (TDD) and Behavior-Driven Development (BDD) practices to ensure high-quality software development.</li>",
        "<li><b>Agile Methodology</b>: I utilized SCRUM as the primary methodology for project management, fostering an agile and efficient development process</li>.",
        "</ul>"
      ],
      "links": [
        {
          "tag": "Website",
          "url": "https://competition-policy.ec.europa.eu/index_en",
          "text": "EU, DG COMP Website"
        }
      ],
      "projects": [
        {
          "name": "Definition of Two RESTful APIs",
          "description": [
            "<P>The first project involved defining two RESTful APIs that implemented the Richardson Maturity Model Level 3, incorporating HATEOAS. These APIs were designed as platforms for the secure exchange of legal documents in anti-cartel proceedings.</P>",
            "<ul>",
            "<li><b>Internal API</b>: The first API was intended to serve the internal staff of the European Commission.</li>",
            "<li><b>External API</b>: The second API served as a platform for external personnel.</li>",
            "</ul>",
            "Both APIs featured access control through a Single Sign-On system called CAS. Data persistence was managed using both Oracle DB and MongoDB. Additionally, the APIs were integrated with an internal secure document storage project via a SOAP interface. The workflow of each procedure was managed using the State design pattern, and system administrators were notified of status changes via email."
          ],
          "links": [
            {
              "tag": "Documentation",
              "url": "https://competition-policy.ec.europa.eu/document/download/2fc568de-61ff-4aaf-9c9b-e03e96324ef7_en?filename=eleniency_guidance_access-notification.pdf",
              "text": "e-leniency documentation"
            }
          ],
          "technologies": [
            "Java",
            "Spring Security",
            "Spring MVC",
            "Spring Data",
            "Spring Documentation",
            "Spring IoC",
            "JPA",
            "Eclipselink",
            "WebLogic 12",
            "Oracle DB",
            "MongoDB",
            "Swagger",
            "JUnit",
            "AngularJS",
            "Selenium",
            "Maven",
            "GIT",
            "CAS",
            "Jira",
            "JUnit",
            "Bamboo",
            "SonarQube",
            "TDD",
            "BDD"
          ]
        },
        {
          "name": "E-Confidenciality",
          "description": [
            "<P>In the second project, I played a pivotal role as the software architect, overseeing the development of a sophisticated system for negotiating confidential versions of legal documentation in anti-cartel proceedings. My contributions were instrumental in establishing and assigning dynamic verification groups responsible for managing online legal processes. These groups, composed of internal personnel, were verified through the Central Authentication Service (CAS), a system previously implemented under my guidance.</P>",
            "<P>I architected the system that governs the status of each document using the Status design pattern, which allows for the reversal of changes, ensuring the integrity and accuracy of legal documents throughout the negotiation process. My expertise ensured that any modifications could be undone, maintaining the documents' fidelity.</P>",
            "<P>Additionally, I selected and integrated Oracle DB and MongoDB as the database engines, capitalizing on their strengths to efficiently manage the project's complex data requirements. I ensured that the implementation followed the Richardson Maturity Model Level 3, incorporating HATEOAS (Hypermedia as the Engine of Application State) principles. This approach guaranteed a robust, scalable system capable of supporting the intricate workflows involved in anti-cartel legal proceedings.</P>",
            "<P>Through my leadership and technical expertise, I significantly enhanced the project's capability to provide a secure, reliable, and user-friendly platform for negotiating sensitive legal documents, thus bolstering the efficacy of anti-cartel enforcement actions. My work as the software architect was crucial in achieving the project's goals and ensuring its success.</P>"
          ],
          "links": [
            {
              "tag": "Documentation",
              "url": "https://competition-policy.ec.europa.eu/index/it-tools/econfidentiality_en",
              "text": "e-confidenciality documentation"
            }
          ],
          "technologies": [
            "AngularJS",
            "Ansible",
            "BDD",
            "Bamboo",
            "CAS",
            "Docker",
            "Eclipselink",
            "Enterprise Architect",
            "GIT",
            "IntelliJ IDEA",
            "JPA",
            "JUnit",
            "Java",
            "Jira",
            "Maven",
            "MongoDB",
            "Oracle DB",
            "Selenium",
            "SonarQube",
            "Spring Data",
            "Spring Documentation",
            "Spring IoC",
            "Spring MVC",
            "Spring Security",
            "Swagger",
            "TDD",
            "WebLogic 12"
          ]
        },
        {
          "name": "Semantic index of legal documents",
          "description": [
            "<P>The third project, proposed by me, centers around developing a sophisticated system for semantic indexing of legal documents. I acted as the architect for this project, deeply involving myself in the programming process to ensure its success. The project utilizes clustering techniques, specifically the K-Means algorithm, applied to the dataset of legal documents from the anti-cartel department. The indexing process is integrated into a microservice designed with the gRPC framework, implemented in C++14 to ensure maximum efficiency and performance.</P>",
            "<P>The architecture of these microservices follows the Command Query Responsibility Segregation (CQRS) paradigm, deployed on a service pool to guarantee resilience and rapid response times. The semantic search capability is powered by a neural network trained on the anti-cartel legal documents dataset, utilizing the TensorFlow framework. Additionally, I developed a complementary service in Java, which leverages this semantic search tool to enhance its functionality.</P>",
            "<P>This dual-service approach not only facilitates high-speed semantic searches but also ensures robust and scalable indexing of complex legal documents. The system's design emphasizes resilience, efficiency, and accuracy, making it a vital tool for legal document management and retrieval in the anti-cartel department. My initiative and hands-on involvement in both the architecture and programming were crucial to the project's success.</P>"
          ],
          "links": [],
          "technologies": [
            "AngularJS",
            "Ansible",
            "Bamboo",
            "C++",
            "CUDA",
            "Cassandra",
            "Docker",
            "Eclipselink",
            "Enterprise Architect",
            "GIT",
            "GoogleTest",
            "Groovy",
            "IntelliJ IDEA",
            "JPA",
            "JUnit",
            "Java",
            "Jira",
            "Maven",
            "Microservices",
            "OpenCL",
            "Oracle DB",
            "Selenium",
            "SonarQube",
            "Spring Data",
            "Spring Documentation",
            "Spring MVC",
            "Spring Security",
            "Swagger",
            "TensorFlow",
            "Valgrind",
            "WebLogic 12",
            "gRPC"
          ]
        },
        {
          "name": "Development of a Renewable Energy Distribution System",
          "description": [
            "<P>The fourth project involves the development of a renewable energy distribution system within the electrical grid, aiming to normalize and stabilize electric consumption over time to maximize the utilization of renewable energies. I played a significant role in this project, contributing extensively to both its architecture and implementation.</P>",
            "<P>The system integrates a diverse array of technologies to achieve its goals. At the core of the communication system lies a Kafka-based messenger cluster, where communication with devices is managed through Avro schemas. An intermediary \"gateway\" is implemented using RaspberryPi, responsible for sending telemetry data and receiving commands. This gateway system is developed in Go, utilizing the ModBus communications protocol in both its serial and TCP versions, with TCP communications secured via a VPN.</P>",
            "<P>The software management for this project is handled through the BalenaIO platform, which facilitates the deployment and operation of the gateway devices. Continuous Integration and Continuous Deployment (CI/CD) processes are managed using CircleCI, ensuring a seamless and efficient development workflow.</P>",
            "<P>In addition, I created a statistics parser in Groovy for the ingestion of telemetry data, providing critical insights into energy consumption patterns and system performance. My involvement was pivotal, overseeing the architecture, implementation, and integration of various components to ensure the project's success.</P>",
            "<P>My hands-on approach and technical expertise were instrumental in addressing the challenges of renewable energy distribution. My contribution in leveraging advanced technologies and developing robust communication protocols underscores my significant role in this innovative project aimed at enhancing the stability and efficiency of renewable energy usage within the electrical grid.</P>"
          ],
          "links": [],
          "technologies": [
            "Go",
            "Unit Test",
            "BalenaIO",
            "RaspberryPI",
            "Modbus",
            "Python",
            "Makefile",
            "Docker",
            "Kubernetes",
            "GCP",
            "Azure",
            "IoTHub",
            "Kafka",
            "Avro Schemas",
            "Ansible",
            "Elastic Search",
            "Kibana",
            "Makefile",
            "Jira",
            "Groovy"
          ]
        }
      ],
      "images": [
        "European_Commission_02.png"
      ]
    },
    {
      "name": "Panel Systems",
      "short_description": [
        "Panel Sistemas is a company specializing in technological solutions, offering software services, consulting, and development to various industries."
      ],
      "thumbnail": "panel_sistemas.png",
      "period_time": "September 2015 - June 2017",
      "full_description": [
        "<P>Panel Sistemas is a leading technology solutions company specializing in software development, consulting, and IT services. Founded in Spain, it serves diverse industries with notable clients such as TelefÃ³nica, BBVA, Santander, Repsol, and El Corte InglÃ©s.</P>",
        "<b>Services Offered</b>",
        "<lu>",
        "<li>Software Development: Custom software solutions.</li>",
        "<li>Consulting: IT strategy and optimization.</li>",
        "<li>Maintenance and Support: Ongoing application support.</li>",
        "<li>Quality Assurance: Comprehensive software testing.</li>",
        "<li>Digital Transformation: Transitioning businesses to digital platforms.</li>",
        "</lu>",
        "<b>Industry Expertise</b>",
        "<lu>",
        "<li>Finance</li>",
        "<li>Telecommunications</li>",
        "<li>Energy</li>",
        "<li>Retail</li>",
        "</lu>"
      ],
      "contribution": [
        "<P>At Panel Sistemas, I worked as a functional analyst on three projects. In addition, I performed tasks as a senior programmer in the post-decision-making phase. My responsibilities as a functional analyst included the following:</P>",
        "<ul>",
        "<li>Analysis of non-functional requirements and decision-making on the exact implementation of the solution (Redmine)</li>",
        "<li>Creation of sub-tasks with lower granularity to achieve objectives (Redmine)</li>",
        "<li>Time estimation (Redmine)</li>",
        "<li>Definition and implementation of the project structure in the specified technologies</li>",
        "<li>Definition of unit test coverage and quality (JUnit, Google Test, Valgrind)</li>",
        "<li>Implementation of CI/CD systems (Jenkins)</li>",
        "<li>Utilization of SCRUM as the development methodology</li>",
        "<li>Application of TDD and BDD in software development</li>",
        "<li>Delivery of training sessions on TDD, BDD, Neural Networks, Clean Code, and more</li>",
        "</ul>"
      ],
      "links": [
        {
          "tag": "Website",
          "url": "https://www.panel.es/",
          "text": "Panel Sistemas Website"
        }
      ],
      "projects": [
        {
          "name": "Modernizing Maritime and Air Transport Messaging: Replacing IATA with XML/JSON",
          "description": [
            "<P>The first of the projects I worked on involved implementing a message exchange platform for maritime and air transport. This project successfully replaced the outdated IATA standard, which was based on plain text and prone to human errors due to its reliance on the position of the text string, with a more modern, readable, and error-resistant format based on XML/JSON.</P>",
            "<P>To achieve this, I defined a parser that translated messages from the old format to either of the two modern standards through a Restful API that adhered to the Richardson Level 3 maturity model. This API not only provided an entry point for message translation but also facilitated intercommunication with other platforms worldwide and the storage of these communications to ensure the traceability of package transit globally. The project's goals required a high-availability structure and a replicated database system based on MongoDB, implemented through a cluster of servers with each shard replicated.</P>"
          ],
          "links": [],
          "technologies": [
            "Java",
            "Spring Security",
            "Spring MVC",
            "Spring Data",
            "Spring Documentation",
            "JPA",
            "Hibernate",
            "Wildfly/JBoss",
            "MongoDB",
            "RabbitMQ",
            "JUnit",
            "JSF",
            "Primefaces",
            "GWT",
            "AWS",
            "Phonegap",
            "Hadoop",
            "Selenium",
            "Maven",
            "GIT",
            "Jenkins",
            "Redmine",
            "IntelliJ IDEA",
            "Docker",
            "Ansible"
          ]
        },
        {
          "name": "Iberdrola's Intelligent Terminal Management with Siemens",
          "description": [
            "The second project was a joint project with Siemens to create Iberdrola's intelligent terminal management infrastructure. These terminals took information about the user's electricity consumption by sending a daily report to the server using the SNMP communication protocol, unless a more detailed monitoring was desired on a specific device or set of devices, in which case reports would be generated with a level of detail that would reach consumption per minute. These data were stored in a non-relational MongoDB database, which for obvious reasons were several clusters, with load balancing and replicated shards. The information of each point was stored in the cluster by geographical proximity, as long as the server was able to assume the volume of data that included the inclusion of said node. The information of each node was kept for one year with a level of detail of consumption per minute, after the year was compacted generating the maximum, minimum and average of each day and this information was kept at this level of detail for 5 years and past That time lapse the information from each node was compacted again. The system also gave the possibility to control and listen in real time to the device through the SNMP and JNLP protocol. This structure was created using microservices in C ++ with the remote procedure call framework gRPC and was implemented in a set of servers based on Red Hat Enterprise Linux."
          ],
          "links": [],
          "technologies": [
            "C++",
            "Google Test",
            "Valgrind",
            "cLion",
            "Boost",
            "gRPC",
            "Microservices",
            "Java",
            "RabbitMQ",
            "JPA",
            "Hibernate",
            "Apache Tomcat",
            "MongoDB",
            "JUnit",
            "JSF",
            "Primefaces",
            "Hadoop",
            "Maven",
            "GIT",
            "Jenkins",
            "Jira",
            "IntelliJ IDEA",
            "Docker",
            "Ansible",
            "Kubernetes",
            "JNLP",
            "SNMP",
            "Spring Security",
            "Spring MVC",
            "Spring Data",
            "Spring Documentation",
            "Spring IOT"
          ]
        },
        {
          "name": "Monitoring and Care System for People with Disabilities or the Elderly for Securitas Direct",
          "description": [
            "<P>The third project, executed for Securitas Direct, involved creating a comprehensive monitoring and care system for people with disabilities or the elderly. The system identified the user's status based on various sensors distributed throughout their home, including presence sensors, door opening sensors, and personal fall sensors. This data was collected to provide detailed information about the user's activities, which was not only stored and accessible for consultation but also capable of generating alarms based on the individual's typical behavior patterns. The system analyzed these patterns and alerted if any sudden changes were detected, with customizable alarm settings.</P>",
            "<P>Additionally, the application included a mobile component developed as a hybrid application. This mobile version utilized sensors to identify falls, track the GPS position of the individual, and more.</P>"
          ],
          "links": [],
          "technologies": [
            "C++",
            "Google Test",
            "Valgrind",
            "cLion",
            "Boost",
            "gRPC",
            "Microservices",
            "Java",
            "AWS",
            "Spring Security",
            "Spring MVC",
            "Spring Data",
            "Spring Documentation",
            "Spring IoC",
            "Spring Batch",
            "RabbitMQ",
            "JPA",
            "Hibernate",
            "Apache Tomcat",
            "Oracle DB",
            "JUnit",
            "Ionic",
            "Typescript",
            "Hadoop",
            "Selenium",
            "Jasmine",
            "Maven",
            "GIT",
            "Jenkins",
            "Jira",
            "Confluence",
            "IntelliJ IDEA",
            "Docker",
            "Ansible",
            "Kubernetes"
          ]
        }
      ],
      "images": [
        "iata.png",
        "iberdrola.svg",
        "siemens.png"
      ]
    },
    {
      "name": "Prisa Group",
      "short_description": [
        "Grupo PRISA is a Spanish media conglomerate, renowned for its influential newspapers, radio stations, and educational publications."
      ],
      "thumbnail": "prisa.png",
      "period_time": "March 2012 - May 2015",
      "full_description": [
        "<p>Grupo PRISA, formally known as Promotora de Informaciones, S.A., is a prominent Spanish media conglomerate with significant influence in the Spanish-speaking world. Founded in 1972 by JesÃºs de Polanco, PRISA has grown to become one of the largest media companies in Spain and Latin America, encompassing a wide range of media outlets including newspapers, radio stations, television channels, and educational publications.</p>",
        "<p>Key aspects of Grupo PRISA include:</p>",
        "<ul>",
        "<li><strong>Newspapers:</strong> PRISA's flagship newspaper is <em>El PaÃ­s</em>, one of Spain's most widely read and respected daily newspapers. Launched in 1976 during Spain's transition to democracy, <em>El PaÃ­s</em> quickly became a symbol of the new democratic era, offering high-quality journalism and a progressive editorial stance. The newspaper covers a broad spectrum of topics, from national and international news to culture, economy, and sports. It has also expanded its reach with a robust online presence, catering to millions of readers worldwide.</li>",
        "<li><strong>Radio:</strong> In the radio sector, PRISA owns Cadena SER, Spain's leading radio network. Cadena SER is renowned for its news programming, talk shows, and sports coverage. It has a substantial listenership and is known for its comprehensive news coverage and influential talk shows that address current affairs, politics, and social issues. Other notable radio stations under PRISA's umbrella include Los 40 Principales, a popular music station that plays contemporary hits, and Cadena Dial, which focuses on Spanish-language music.</li>",
        "<li><strong>Television:</strong> PRISA has also ventured into television, with its channel <em>CNN+</em> having been a significant player in the Spanish news market until its closure in 2010. Despite this, PRISA continues to have a presence in the television sector through various partnerships and content production.</li>",
        "<li><strong>Educational Publications:</strong> Beyond news and entertainment, PRISA is a major player in the educational publishing industry through its subsidiary, Santillana. Founded in 1960, Santillana is a leading publisher of educational materials in Spain and Latin America. It offers a wide range of textbooks, digital content, and educational resources for students and teachers, making significant contributions to education in the Spanish-speaking world.</li>",
        "<li><strong>Digital Transformation:</strong> In recent years, PRISA has embraced digital transformation, adapting to the changing media landscape. The company has invested heavily in digital platforms, mobile applications, and online content to reach a broader audience and stay relevant in the digital age. <em>El PaÃ­s</em>, for instance, has a strong digital presence, with a website that attracts millions of visitors each month and a variety of multimedia content, including videos, podcasts, and interactive features.</li>",
        "<li><strong>Global Influence:</strong> PRISA's influence extends beyond Spain and Latin America. It has formed strategic partnerships with international media companies and expanded its reach to other regions. The company's content is consumed by millions of people around the world, making it a significant cultural and informational bridge between Spain, Latin America, and the rest of the world.</li>",
        "</ul>"
      ],
      "contribution": [
        "<p>I have extensive experience working as a remote senior programmer on three significant projects. In this role, I was entrusted with a variety of critical responsibilities that required both technical expertise and strong communication skills. One of my primary duties was attending meetings with the product owner to define detailed project requirements. This involved a thorough analysis of the project goals and breaking them down into specific, actionable tasks. By doing so, I ensured that all aspects of the project were clearly understood and addressed from the outset.</p>",
        "<p>Coordination with external teams was another major aspect of my role. I worked closely with collaboration teams based in South America, India, and China. This international coordination required me to manage different time zones and cultural differences effectively. To facilitate smooth communication and collaboration, I translated and interpreted requirements in Chinese. This language skill was crucial in bridging communication gaps and ensuring that all team members, regardless of their location, had a clear and accurate understanding of the project requirements.</p>",
        "<p>Innovation and efficiency were key focus areas in my role. I participated in brainstorming sessions to develop and implement new tools aimed at improving the automation and productivity of the team. These tools were designed to streamline workflows, reduce manual effort, and enhance the overall efficiency of the development process. By introducing these innovations, I was able to contribute to a more productive and effective team environment.</p>",
        "<p>Detailed project planning was another critical responsibility. I utilized the SCRUM methodology to ensure that all tasks were organized and managed effectively throughout the project lifecycle. This involved creating detailed plans, setting realistic timelines, and continuously monitoring progress to ensure that project milestones were met. The use of SCRUM allowed for a flexible and iterative approach to project management, enabling the team to adapt to changes and address challenges promptly.</p>",
        "<p>Overall, my experience as a remote senior programmer has equipped me with a diverse skill set and a deep understanding of effective project management in a global, remote working environment. My ability to define detailed requirements, coordinate with international teams, innovate for improved productivity, and manage projects using SCRUM has enabled me to contribute significantly to the success of the projects I have been involved in.</p>"
      ],
      "links": [
        {
          "tag": "Website",
          "url": "https://www.prisa.com/es",
          "text": "Prisa Group Website"
        }
      ],
      "projects": [
        {
          "name": "Educational Package Viewer Application",
          "description": [
            "<p>The first project involved developing a comprehensive application for both Android and iOS platforms aimed at viewing educational packages. This application was meticulously designed to be compatible with low-end devices, ensuring that it could reach a broader user base without compromising performance. It was essential for the application to interact fluidly with users, providing a seamless and engaging experience. Furthermore, the application was equipped with functionality to track and send users' progress to the server, facilitating effective monitoring and assessment of their learning journey.</p>",
            "<p>A key feature of the application was its ability to handle educational packages that adhered to the SCORM (Sharable Content Object Reference Model) standard, which is widely used in the field of e-learning for creating and delivering interactive and adaptive learning content. To ensure backward compatibility, the application was also designed to support older versions of educational packages. This dual compatibility ensured that users could access a wide range of educational content, both modern and legacy, thus enhancing the application's utility and appeal in various educational contexts.</p>"
          ],
          "links": [],
          "technologies": [
            "Java",
            "Android Studio",
            "XCode",
            "Objective-C",
            "Dagger",
            "SCORM",
            "HTML5",
            "JavaScript",
            "Bootstrap",
            "jQuery",
            "Jira",
            "GIT",
            "Confluence"
          ]
        },
        {
          "name": "Educational Package Standardization Project",
          "description": [
            "<p>The second project emerged from the necessity to standardize educational packages. In an effort to save costs associated with migrating from Flash to HTML5 under the SCORM standard, a decision was made to develop a Flash to HTML5 translation tool. This tool facilitated the conversion of educational content from Flash to HTML5, ensuring compatibility with the SCORM standard. By maintaining a centralized library, the tool allowed any updates to the core system to be applied seamlessly to all previously created packages. This approach significantly streamlined the update process, ensuring consistency and reducing the need for repetitive manual adjustments.</p>",
            "<p>Prior to accepting a new version of the centralized library, thorough testing was conducted in a virtualized environment. Utilizing technologies such as QEmu, KVM, and Xen, the new versions were rigorously evaluated to ensure their stability and functionality. This comprehensive testing process was crucial in identifying and resolving potential issues before deployment, thereby maintaining the integrity of the educational packages. The project not only achieved cost savings and improved efficiency but also provided a robust framework for future updates and enhancements, ensuring the longevity and adaptability of the educational content.</p>"
          ],
          "links": [],
          "technologies": [
            "C++",
            "Linux",
            "Flash",
            "HTML5",
            "JavaScript",
            "AngularJS",
            "Bootstrap",
            "SCORM",
            "jQuery",
            "Jira",
            "GIT",
            "Confluence",
            "QEmu",
            "KVM",
            "Xen",
            "Bash",
            "NginX",
            "Valgrind"
          ]
        },
        {
          "name": "Educational Platform Project",
          "description": [
            "<p>The third project was developed using C# on the .NET platform, designed as a comprehensive environment for teachers and education professionals. This platform enabled users to manage various educational tasks, including student management, handling educational packages, and analyzing their results. It also facilitated the scheduling and execution of tasks related to the media server for streaming purposes.</p>",
            "<p>Furthermore, the platform featured a high-resolution audio and video communication center via IP, enhancing real-time interactions between educators and students. This robust system supported a seamless and efficient workflow, ensuring that all educational activities were effectively managed and executed within a single unified environment.</p>"
          ],
          "links": [],
          "technologies": [
            "C#",
            ".NET",
            "IIS",
            "Windows Server",
            "Microsoft SQL Server",
            "Jira",
            "GIT",
            "Confluence"
          ]
        }
      ],
      "images": []
    }
  ],
  "educations": {
    "university": [
      {
        "university_name": "AlcalÃ¡ de Henares University",
        "title": "Master's Degree in Artificial Intelligence in information and communication technologies",
        "period_time": "September 2007 - June 2009",
        "summary": [
          "Artificial intelligence and machine learning. Supervised, unsupervised and reinforced. Symbolic learning Classification and regression models. Optimization of models.",
          "Deep nets. Multilayer networks, backpropagation. Loss functions. Hyperparameters and learning strategies.",
          "Convolutional networks. Recognition of images.",
          "Sequential, recurrent networks. LSTM models.",
          "Paralleling and computation techniques in GPU-based architectures.",
          "Vectorization techniques. Programming with tensorflow and theano.",
          "Scalable automated learning. Parallelization frameworks in computer cluster.",
          "Applications in medicine, finance, automatic driving, etc."
        ],
        "images": [
          "uah.svg"
        ],
        "thumbnail": "uah.svg"
      },
      {
        "university_name": "AlcalÃ¡ de Henares University",
        "title": "Technical Engineering in Computer Science",
        "period_time": "September 2001 - June 2007",
        "summary": [
          "With honors in: Artificial Intelligence, Fuzzy Logic, Artificial Vision Systems, Physics II, Mathematical Analysis, Data Structure, Networks."
        ],
        "images": [
          "uah.svg"
        ],
        "thumbnail": "uah.svg"
      }
    ],
    "complementary": [
      {
        "institution": "Linux Foundation",
        "title": "LPIC - 2",
        "period_time": "September 2015",
        "summary": [
          "Official certification of Linux system administration that implies the qualification for:",
          "Advanced administration of Linux systems, including those related to the kernel.",
          "Perform advanced management of storage of blocks and file system, as well asnetworks advanced authentication and system security, including firewall and VPN",
          "Install and configure core network services (DHCP, DNS, SSH, web servers, file servers, email, etc).",
          "Monitor assistants and advise management on automation and purchases"
        ],
        "images": [
          "linux_foundation.png"
        ],
        "thumbnail": "linux_foundation.png"
      },
      {
        "institution": "Linux Foundation",
        "title": "LPIC - 1",
        "period_time": "September 2013",
        "summary": [
          "Official certification of Linux system administration that implies the qualification for:",
          "Install a Linux system, including X11 and configure it as a network client.",
          "Work in command line.",
          "Manage files and access permissions including ACL, as well as system security.",
          "Perform simple maintenance tasks."
        ],
        "images": [
          "linux_foundation.png"
        ],
        "thumbnail": "linux_foundation.png"
      },
      {
        "institution": "AlcalÃ¡ de Henares University",
        "title": "Design and evaluation of digital educational contents",
        "period_time": "September 2007",
        "summary": [
          "Design of educational content through the SCORM standard."
        ],
        "images": [
          "uah.svg"
        ],
        "thumbnail": "uah.svg"
      },
      {
        "institution": "AlcalÃ¡ de Henares University",
        "title": "Course of artificial vision",
        "period_time": "September 2007",
        "summary": [
          "Systems of artificial vision, convolutional matrices, filters."
        ],
        "images": [
          "uah.svg"
        ],
        "thumbnail": "uah.svg"
      }
    ],
    "languages": [
      {
        "language": "Spanish",
        "spoken": "Native",
        "writen": "Native",
        "read": "Native",
        "acreditations": [],
        "thumbnail": "spanish.png"
      },
      {
        "language": "English",
        "spoken": "Fluent",
        "writen": "Fluent",
        "read": "Fluent",
        "acreditations": [],
        "thumbnail": "english.svg"
      },
      {
        "language": "Chinese",
        "spoken": "Fluent",
        "writen": "Fluent",
        "read": "Fluent",
        "acreditations": [
          {
            "institution": "Confucius Institute Madrid",
            "title": "HSK 3",
            "period_time": "September 2009"
          }
        ],
        "thumbnail": "chinese.png"
      }
    ]
  }
}
