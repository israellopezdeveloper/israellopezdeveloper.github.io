\documentclass[a4paper,10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{fontspec}
\setmainfont{JetBrainsMono Nerd Font}
\usepackage{parskip}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{multirow}
\usepackage{array}
\geometry{top=2cm, bottom=2cm, left=2.5cm, right=2.5cm}
\hypersetup{
  colorlinks=true,
  linkcolor=blue,
  filecolor=magenta,
  urlcolor=blue,
  pdftitle={CV},
  pdfpagemode=FullScreen,
}
\newcommand{\cvsection}[1]{
  \vspace{2mm}
  \begin{tcolorbox}[colback=gray!30, colframe=gray!30, boxrule=0pt, arc=0mm, outer arc=0mm, width=\textwidth, boxsep=0pt, left=2mm, right=2mm]
    \raggedright\textbf{\LARGE{#1}}
  \end{tcolorbox}
  \vspace{2mm}
}
\newcommand{\cvsubsection}[3]{
  \begin{tcolorbox}[colback=gray!20, colframe=gray!20, boxrule=0pt, arc=0mm, outer arc=0mm, width=\dimexpr\textwidth-2mm\relax, boxsep=0pt, left=2mm, right=2mm, top=2mm, bottom=2mm]
    \begin{tabular*}{\dimexpr\textwidth-6mm\relax}{p{0.7\textwidth} @{\extracolsep{\fill}} p{0.3\textwidth}}
      \raggedright
      \textbf{#1} \textit{#2} & \raggedleft \small{#3}
    \end{tabular*}
  \end{tcolorbox}
  \vspace{2mm}
}
\newcommand{\cvsubsubsection}[1]{
\begin{tcolorbox}[colback=gray!10, colframe=gray!10, boxrule=0pt, arc=0mm, outer arc=0mm, width=\textwidth, boxsep=0pt, left=4mm, right=4mm, top=1mm, bottom=1mm]
  \textbf{#1}
\end{tcolorbox}
\vspace{1mm}
}
\newcommand{\cvsubsubsubsection}[1]{
  \begin{tcolorbox}[colback=gray!5, colframe=gray!5, boxrule=0pt, arc=0mm, outer arc=0mm, width=\textwidth, boxsep=0pt, left=6mm, right=6mm, top=1mm, bottom=1mm]
    \textbf{#1}
  \end{tcolorbox}
  \vspace{1mm}
}
\newcommand{\roundedimage}[1]{
  \begin{tikzpicture}
    \clip[rounded corners=5mm] (0,0) rectangle (2.5,2.5);
    \node[anchor=south west,inner sep=0] at (0,0) {\includegraphics[width=2.5cm,height=2.5cm]{#1}};
  \end{tikzpicture}
}
\begin{document}
  \begin{tabular*}{\textwidth}{l @{\extracolsep{\fill}} r}
    \textbf{\Huge Israel López} & \multirow{5}{*}{\roundedimage{./public/images/israel.png}}\\
    \textbf{Ingeniero de Software Senior} & \\ 
     @israellopezdeveloper & \\ 
    󰇮 israel.lopez.developer@gmail.com & \\ 
    󰌻 https://www.linkedin.com/in/israellopezmaiz/ & \\ 
     +34 648 13 66 40 & \\ 
\end{tabular*}
  \cvsection{Summary}
Mi mochila está llena de código y de ganas de seguir aprendiendo. ¡La programación es mi superpoder!

Desde los 13 me picó el gusanillo de la programación y no he podido parar. ¡En cualquier rincón del mundo, con mi portátil bajo el brazo!

Me considero una mente inquieta por las ganas insaciables que tengo de aprender,  innovar y superar nuevos retos intelectuales. Disfruto de la sensación de ver cómo una idea se convierte en realidad a través de código.

Tengo la suerte de que la programación no solo es mi trabajo, también es mi pasión y si hay algo que me caracteriza es mi insistencia por encontrar la solución óptima al proyecto.  Porque si, me  enorgullece entregar código limpio y eficiente.

Si conectamos, ¿estás listo para embarcarte en una nueva aventura tecnológica conmigo?

\cvsection{Experiencia}
\cvsubsection{Ingeniero Software Senior}{Devo}{Marzo 2020 - Marzo 2024}
En Devo, desempeñé un papel crucial en el equipo de ingesta de datos, centrándome específicamente en el balanceador de carga. Dada la sensibilidad de los datos y su importancia para diagnosticar posibles ataques, el servicio debía ser tanto fiable como rápido. Bajo mi experiencia, el sistema pudo manejar un impresionante volumen de ingesta de aproximadamente 60TB por día. Además, contribuí significativamente a los esfuerzos de Devo para adaptarse a los estándares necesarios para la integración con sistemas federales de EE.UU., logrando la certificación FedRAMP. Asimismo, participé en la creación de un sistema de respaldo multirregional y multicloud, asegurando la redundancia y fiabilidad de los datos en diversos entornos. Mi trabajo garantizó que la plataforma de Devo cumpliera con los más altos niveles de seguridad y cumplimiento, mejorando la capacidad de la empresa para servir eficazmente a sus clientes.

\cvsubsubsection{Mejorar el balanceador de carga existente}
Contribuí significativamente a mejorar el rendimiento del balanceador de carga de Devo mediante la implementación de estrategias avanzadas que mejoraron la tolerancia a fallos y optimizaron la monitorización de los servicios de ingesta de datos. Analicé meticulosamente los requisitos del sistema e identifiqué áreas clave de mejora, asegurando que el balanceador de carga pudiera manejar niveles variables de tráfico con mayor eficiencia. Mis esfuerzos en mejorar los mecanismos de tolerancia a fallos ayudaron al sistema a recuperarse rápidamente de fallos inesperados, manteniendo alta disponibilidad y fiabilidad. Además, creé soporte para el protocolo RELP (Reliable Event Logging Protocol) desde cero, asegurando un registro de eventos fiable y eficiente en todo el sistema.

Asimismo, desarrollé métodos para equilibrar la carga basado en el cliente, lo que facilitó consultas de datos más compactas. Al asegurar que los datos no se mezclaran innecesariamente en todos los nodos, agilice el proceso de recuperación de datos, haciéndolo más rápido y eficiente. Este enfoque de balanceo de carga específico para clientes no solo mejoró el rendimiento de las consultas sino que también redujo la sobrecarga en el sistema. Mis contribuciones llevaron a un sistema de balanceo de carga más robusto y receptivo, mejorando significativamente el rendimiento y la experiencia del usuario en Devo.

\cvsubsubsubsection{Tecnologías}
Node.js, C, C++, FIPS, FedRAMP, SonarQube, Snyk, Owasp, Jenkins, Gitlab CI/CD, Kubernetes, GIT, Jira, TDD, BDD

\cvsubsubsection{Crear un sistema de respaldo multicloud}
Encabecé el desarrollo de un sistema de respaldo multicloud de última generación desde cero, un avance crucial para las capacidades de redundancia y recuperación ante desastres de Devo. Este sistema fue diseñado para almacenar registros de clientes en múltiples regiones y entornos en la nube, asegurando la máxima protección y disponibilidad de los datos. Una de las características clave de este sistema fue su capacidad para configurar tipos de almacenamiento en tiempo real, permitiendo ajustes fluidos a las configuraciones de almacenamiento en la nube basados en necesidades y criterios en tiempo real.

Implementé criterios sofisticados de transición de datos, que automatizaron el movimiento de datos entre diversas clases de almacenamiento basándose en patrones de uso y políticas de retención. Esta característica aseguraba que los datos siempre se almacenaran de la manera más rentable y eficiente, sin comprometer la accesibilidad o el rendimiento.

Además de esto, mi sistema incluía mecanismos robustos de recuperación de nodos, permitiendo una rápida restauración del servicio en caso de falla de un nodo. Esto redujo significativamente el tiempo de inactividad y mantuvo la integridad y disponibilidad de los datos críticos. Para mejorar aún más la eficiencia del sistema, desarrollé métodos avanzados de compactación de datos para los datos que estaban cerca del final de su vida útil, optimizando el uso del almacenamiento y reduciendo la sobrecarga.

Finalmente, integré horarios precisos de limpieza del sistema, asegurando que los datos obsoletos se eliminaran oportunamente, manteniendo un rendimiento óptimo del sistema y la salud del almacenamiento. Mis contribuciones a este sistema de respaldo multicloud no solo fortalecieron las capacidades de gestión de datos de Devo sino que también garantizaron el cumplimiento de estrictas regulaciones de seguridad y retención de datos. Este proyecto destacó mi experiencia en la creación de soluciones de almacenamiento de datos resilientes, escalables y altamente eficientes, reforzando el compromiso de Devo con la excelencia en seguridad y análisis de datos.

\cvsubsubsubsection{Tecnologías}
Java, AWS, GCP, FIPS, FedRAMP, SonarQube, Snyk, Owasp, Jenkins, Gitlab CI/CD, Kubernetes, GIT, Jira, TDD, BDD

\cvsubsubsection{Integrar revisiones de calidad y seguridad en el pipeline CI/CD}
Jugué un papel fundamental en la integración de revisiones comprensivas de calidad y seguridad en el pipeline CI/CD de Devo. Esta iniciativa fue crucial para elevar el proceso de desarrollo a cumplir con los estrictos estándares de la industria, incluyendo NIST, FIPS y FedRAMP.

Integración del Pipeline Jenkins

Inicialmente, me centré en incorporar SonarQube en el pipeline Jenkins para el análisis continuo de calidad del código. Configuré SonarQube para realizar análisis estático del código, lo que permitió al equipo detectar olores de código, errores y posibles vulnerabilidades temprano en el ciclo de desarrollo. Esta integración aseguró que solo se fusionara código de alta calidad, reduciendo significativamente la cantidad de problemas que llegaban a producción.

Transición a GitLab CI/CD

Tras el éxito con Jenkins, hice la transición del pipeline a GitLab CI/CD. Aproveché las características avanzadas de GitLab para agilizar el proceso y mejorar la automatización de las implementaciones de código. Este cambio facilitó una mejor colaboración y eficiencia en todo el equipo de desarrollo.

Escaneo de Vulnerabilidades de Seguridad

Para abordar las vulnerabilidades de seguridad, integré Snyk y OWASP en el pipeline CI/CD. Snyk proporcionó escaneos en tiempo real de dependencias de código abierto, identificando y sugiriendo soluciones para vulnerabilidades conocidas. Mientras tanto, se utilizaron herramientas OWASP para realizar evaluaciones de seguridad exhaustivas, enfocándose en identificar y mitigar riesgos asociados con aplicaciones web.

Lograr Estándares de Cumplimiento

Mi trabajo meticuloso aseguró que el pipeline cumpliera con los rigurosos estándares requeridos por NIST, FIPS y FedRAMP. Establecí verificaciones y balances automatizados dentro del proceso CI/CD, garantizando que cada cambio de código pasara por revisiones exhaustivas de calidad y seguridad antes de su implementación. Esto no solo mejoró la postura general de seguridad de las aplicaciones, sino que también agilizó el proceso de verificación de cumplimiento, facilitando que Devo demostrara adherencia a estos estándares críticos.

Resultado

Mis contribuciones mejoraron significativamente la robustez del pipeline CI/CD de Devo, asegurando que la calidad y seguridad del código se mantuvieran en los más altos niveles. La integración de SonarQube, Snyk y herramientas OWASP en el pipeline resultó en un proceso de desarrollo más seguro y eficiente, alineado con el compromiso de la empresa con la innovación y la excelencia. Este proyecto fue un paso crucial para mantener la posición de Devo a la vanguardia de la ciberseguridad y el análisis de datos, proporcionando a los clientes soluciones confiables y seguras.

\cvsubsubsubsection{Tecnologías}
Gitlab CI/CD, Jenkins, NIST, FIPS, FedRAMP, SonarQube, Snyk, Owasp, Kubernetes, Jira

\cvsubsubsection{Creación de un sistema de alerta para monitoreo con Prometheus}
Fui fundamental en el desarrollo de un sistema de alerta integral diseñado para monitorear el rendimiento y situaciones críticas en entornos de producción utilizando Prometheus. Reconociendo la necesidad de monitoreo en tiempo real y mecanismos de respuesta rápida, diseñé una solución que proporcionaba conocimientos detallados sobre el rendimiento del sistema y detectaba posibles problemas como fraudes, ataques y otras anomalías.

Para lograr esto, integré Prometheus en la infraestructura de Devo, configurando mecanismos robustos de recopilación de métricas y alertas. Definí indicadores clave de rendimiento (KPIs) y umbrales cruciales para mantener la salud y estabilidad del sistema. Aprovechando las potentes capacidades de consulta de Prometheus, establecí una serie de reglas de alerta que podían identificar patrones anormales en la ingesta de datos, tiempos de procesamiento y uso de recursos del sistema.

Además de configurar el sistema de alerta, desarrollé una guía completa de solución de problemas para asistir al equipo de operaciones en el diagnóstico y resolución de problemas rápidamente. Esta guía incluía procedimientos paso a paso para identificar la causa raíz de las alertas, acciones recomendadas y protocolos de escalación. La guía aseguraba que los miembros del equipo tuvieran una comprensión clara de cómo manejar diferentes tipos de alertas, desde degradaciones menores del rendimiento hasta fallas críticas del sistema.

Asimismo, implementé acciones automatizadas de respuesta temprana para mitigar posibles problemas antes de que se escalaran. Estas acciones automatizadas incluían reiniciar servicios, redistribuir la carga y escalar recursos dinámicamente según la naturaleza y gravedad de las alertas. Al automatizar estas respuestas iniciales, reduje significativamente el tiempo medio de recuperación (MTTR) y minimicé el impacto de los problemas en los usuarios finales.

Para mejorar la efectividad general del sistema de monitoreo, también diseñé protocolos para manejar situaciones críticas del sistema. Estos protocolos delinearon los roles y responsabilidades de los miembros del equipo, estrategias de comunicación y esfuerzos de coordinación requeridos durante incidentes mayores. Este enfoque estructurado aseguró una respuesta rápida y coordinada, reduciendo el tiempo de inactividad y manteniendo la fiabilidad del servicio.

Mis contribuciones a la creación del sistema de alerta no solo mejoraron las capacidades de monitoreo de la plataforma de Devo, sino que también fortalecieron su postura de seguridad al permitir la detección temprana de fraudes y ataques. Mi trabajo aseguró que el sistema pudiera mantener un alto rendimiento y resiliencia, proporcionando a los clientes de Devo una plataforma de análisis de datos confiable y segura.

\cvsubsubsubsection{Tecnologías}
Prometheus, Grafana, Kubernetes, Docker, Elastic Stack, Kibana, Python, Bash, Ansible, GIT

\cvsubsection{Arquitecto Software}{Comisión Europea}{Junio 2017 - Enero 2020}
En la Comisión Europea, trabajé como arquitecto de software en tres proyectos, con una amplia gama de responsabilidades que demostraron mi experiencia y liderazgo en el desarrollo de software. Mis principales responsabilidades incluyeron:



Gestión de Requisitos No Funcionales y Definición del Proyecto: Usando Enterprise Architect, fui responsable de identificar y gestionar requisitos no funcionales, asegurando que cada proyecto estuviera claramente definido y alineado con los objetivos organizacionales.

Priorización de Tareas: Utilizando Jira, prioricé tareas eficazmente para asegurar que los elementos más críticos se abordaran de manera oportuna.

Gestión de Dependencias de Tareas: Usé Jira para gestionar dependencias entre tareas, asegurando un progreso fluido y minimizando retrasos.

Selección de Tecnología: Fui fundamental en la selección de tecnologías apropiadas para cada proyecto, asegurando que fueran adecuadas para cumplir con las necesidades del proyecto.

Definición de Perfiles de Miembros del Equipo: Definí los perfiles y roles de los miembros del equipo, asegurando que el equipo tuviera las habilidades y experiencia necesarias para tener éxito.

Colaboración entre Equipos: Facilitó la colaboración entre diferentes equipos cuyos proyectos necesitaban integrarse, asegurando una integración y cooperación sin problemas.

Control de Calidad y Pruebas: Definí controles de calidad y protocolos de pruebas, incluyendo pruebas unitarias utilizando tecnologías como JUnit o Google Test/Valgrind, dependiendo del lenguaje de programación.

Implementación de Sistemas CI/CD: Definí e implementé sistemas de Integración Continua y Despliegue Continuo utilizando herramientas como Bamboo y SonarQube.

Metodologías de Desarrollo de Software: Apliqué prácticas de Desarrollo Basado en Pruebas (TDD) y Desarrollo Basado en Comportamiento (BDD) para asegurar un desarrollo de software de alta calidad.

Metodología Ágil: Utilicé SCRUM como la metodología principal para la gestión de proyectos, fomentando un proceso de desarrollo ágil y eficiente.



\cvsubsubsection{Definición de dos APIs RESTful}
El primer proyecto involucró la definición de dos APIs RESTful que implementaron el Modelo de Madurez de Richardson Nivel 3, incorporando HATEOAS. Estas APIs fueron diseñadas como plataformas para el intercambio seguro de documentos legales en procedimientos antimonopolio.



API Interna: La primera API estaba destinada a servir al personal interno de la Comisión Europea.

API Externa: La segunda API sirvió como plataforma para el personal externo.



Ambas APIs contaban con control de acceso a través de un sistema de Inicio de Sesión Único llamado CAS. La persistencia de datos se gestionó utilizando tanto Oracle DB como MongoDB. Además, las APIs se integraron con un proyecto interno de almacenamiento seguro de documentos a través de una interfaz SOAP. El flujo de trabajo de cada procedimiento se gestionó utilizando el patrón de diseño State, y los administradores del sistema fueron notificados de los cambios de estado por correo electrónico.

\cvsubsubsubsection{Tecnologías}
Java, Spring Security, Spring MVC, Spring Data, Spring Documentation, Spring IoC, JPA, Eclipselink, WebLogic 12, Oracle DB, MongoDB, Swagger, JUnit, AngularJS, Selenium, Maven, GIT, CAS, Jira, JUnit, Bamboo, SonarQube, TDD, BDD

\cvsubsubsection{E-Confidencialidad}
En el segundo proyecto, desempeñé un papel crucial como arquitecto de software, supervisando el desarrollo de un sistema sofisticado para negociar versiones confidenciales de documentación legal en procedimientos antimonopolio. Mis contribuciones fueron fundamentales para establecer y asignar grupos de verificación dinámicos responsables de gestionar procesos legales en línea. Estos grupos, compuestos por personal interno, fueron verificados a través del Servicio Central de Autenticación (CAS), un sistema previamente implementado bajo mi dirección.

Diseñé la arquitectura del sistema que rige el estado de cada documento utilizando el patrón de diseño Status, lo que permite la reversión de cambios, asegurando la integridad y precisión de los documentos legales durante el proceso de negociación. Mi experiencia aseguró que cualquier modificación pudiera deshacerse, manteniendo la fidelidad de los documentos.

Además, seleccioné e integré Oracle DB y MongoDB como motores de base de datos, capitalizando sus fortalezas para gestionar eficientemente los complejos requisitos de datos del proyecto. Aseguré que la implementación siguiera el Modelo de Madurez de Richardson Nivel 3, incorporando principios de HATEOAS (Hypermedia as the Engine of Application State). Este enfoque garantizó un sistema robusto y escalable capaz de soportar los intrincados flujos de trabajo involucrados en los procedimientos legales antimonopolio.

A través de mi liderazgo y experiencia técnica, mejoré significativamente la capacidad del proyecto para proporcionar una plataforma segura, confiable y fácil de usar para negociar documentos legales sensibles, fortaleciendo así la eficacia de las acciones de aplicación antimonopolio. Mi trabajo como arquitecto de software fue crucial para lograr los objetivos del proyecto y asegurar su éxito.

\cvsubsubsubsection{Tecnologías}
AngularJS, Ansible, BDD, Bamboo, CAS, Docker, Eclipselink, Enterprise Architect, GIT, IntelliJ IDEA, JPA, JUnit, Java, Jira, Maven, MongoDB, Oracle DB, Selenium, SonarQube, Spring Data, Spring Documentation, Spring IoC, Spring MVC, Spring Security, Swagger, TDD, WebLogic 12

\cvsubsubsection{Índice semántico de documentos legales}
El tercer proyecto, propuesto por mí, se centra en desarrollar un sistema sofisticado para la indexación semántica de documentos legales. Actué como arquitecto de este proyecto, involucrándome profundamente en el proceso de programación para asegurar su éxito. El proyecto utiliza técnicas de clustering, específicamente el algoritmo K-Means, aplicadas al conjunto de datos de documentos legales del departamento antimonopolio. El proceso de indexación está integrado en un microservicio diseñado con el framework gRPC, implementado en C++14 para asegurar la máxima eficiencia y rendimiento.

La arquitectura de estos microservicios sigue el paradigma de Segregación de Responsabilidad de Comandos y Consultas (CQRS), desplegado en un pool de servicios para garantizar la resiliencia y tiempos de respuesta rápidos. La capacidad de búsqueda semántica está potenciada por una red neuronal entrenada en el conjunto de datos de documentos legales antimonopolio, utilizando el framework TensorFlow. Además, desarrollé un servicio complementario en Java, que aprovecha esta herramienta de búsqueda semántica para mejorar su funcionalidad.

Este enfoque dual de servicios no solo facilita búsquedas semánticas de alta velocidad, sino que también asegura una indexación robusta y escalable de documentos legales complejos. El diseño del sistema enfatiza la resiliencia, eficiencia y precisión, convirtiéndolo en una herramienta vital para la gestión y recuperación de documentos legales en el departamento antimonopolio. Mi iniciativa y participación activa en la arquitectura y programación fueron cruciales para el éxito del proyecto.

\cvsubsubsubsection{Tecnologías}
AngularJS, Ansible, Bamboo, C++, CUDA, Cassandra, Docker, Eclipselink, Enterprise Architect, GIT, Google Test, Groovy, IntelliJ IDEA, JPA, JUnit, Java, Jira, Maven, Microservices, OpenCL, Oracle DB, Selenium, SonarQube, Spring Data, Spring Documentation, Spring MVC, Spring Security, Swagger, TensorFlow, Valgrind, WebLogic 12, gRPC

\cvsubsubsection{Desarrollo de un sistema de distribución de energía renovable}
El cuarto proyecto implica el desarrollo de un sistema de distribución de energía renovable dentro de la red eléctrica, con el objetivo de normalizar y estabilizar el consumo eléctrico a lo largo del tiempo para maximizar la utilización de energías renovables. Desempeñé un papel significativo en este proyecto, contribuyendo extensamente tanto a su arquitectura como a su implementación.

El sistema integra una amplia variedad de tecnologías para lograr sus objetivos. En el núcleo del sistema de comunicación se encuentra un clúster de mensajería basado en Kafka, donde la comunicación con dispositivos se gestiona a través de esquemas Avro. Un "gateway" intermediario está implementado utilizando Raspberry Pi, responsable de enviar datos de telemetría y recibir comandos. Este sistema de gateway se desarrolló en Go, utilizando el protocolo de comunicaciones ModBus en sus versiones serial y TCP, con las comunicaciones TCP aseguradas a través de una VPN.

La gestión del software para este proyecto se maneja a través de la plataforma BalenaIO, que facilita el despliegue y operación de los dispositivos gateway. Los procesos de Integración Continua y Despliegue Continuo (CI/CD) se gestionan utilizando CircleCI, asegurando un flujo de trabajo de desarrollo eficiente y sin interrupciones.

Además, creé un analizador de estadísticas en Groovy para la ingesta de datos de telemetría, proporcionando información crítica sobre patrones de consumo de energía y rendimiento del sistema. Mi participación fue fundamental, supervisando la arquitectura, implementación e integración de varios componentes para asegurar el éxito del proyecto.

Mi enfoque práctico y experiencia técnica fueron esenciales para abordar los desafíos de la distribución de energía renovable. Mi contribución al aprovechar tecnologías avanzadas y desarrollar protocolos de comunicación robustos subraya mi papel significativo en este innovador proyecto destinado a mejorar la estabilidad y eficiencia del uso de energía renovable dentro de la red eléctrica.

\cvsubsubsubsection{Tecnologías}
Go, Unit Test, BalenaIO, RaspberryPI, Modbus, Python, Makefile, Docker, Kubernetes, GCP, Azure, IoTHub, Kafka, Avro Schemas, Ansible, Elastic Search, Kibana, Makefile, Jira, Groovy

\cvsubsection{Ingeniero Software Senior}{Panel Sistemas}{Septiembre 2015 - Junio 2017}
En Panel Sistemas, trabajé como analista funcional en tres proyectos. Además, realicé tareas como programador senior en la fase posterior a la toma de decisiones. Mis responsabilidades como analista funcional incluyeron lo siguiente:



Análisis de requisitos no funcionales y toma de decisiones sobre la implementación exacta de la solución (Redmine)

Creación de sub-tareas con menor granularidad para lograr objetivos (Redmine)

Estimación de tiempos (Redmine)

Definición e implementación de la estructura del proyecto en las tecnologías especificadas

Definición de la cobertura y calidad de las pruebas unitarias (JUnit, Google Test, Valgrind)

Implementación de sistemas CI/CD (Jenkins)

Utilización de SCRUM como metodología de desarrollo

Aplicación de TDD y BDD en el desarrollo de software

Impartición de sesiones de capacitación sobre TDD, BDD, Redes Neuronales, Código Limpio y más



\cvsubsubsection{Modernización de la mensajería de transporte marítimo y aéreo: reemplazo de IATA por XML/JSON}
El primero de los proyectos en los que trabajé involucró la implementación de una plataforma de intercambio de mensajes para el transporte marítimo y aéreo. Este proyecto reemplazó con éxito el estándar IATA obsoleto, basado en texto plano y propenso a errores humanos debido a su dependencia de la posición de la cadena de texto, con un formato más moderno, legible y resistente a errores basado en XML/JSON.

Para lograr esto, definí un analizador que traducía mensajes del formato antiguo a cualquiera de los dos estándares modernos a través de una API Restful que cumplía con el modelo de madurez de Richardson Nivel 3. Esta API no solo proporcionó un punto de entrada para la traducción de mensajes, sino que también facilitó la intercomunicación con otras plataformas en todo el mundo y el almacenamiento de estas comunicaciones para asegurar la trazabilidad del tránsito de paquetes a nivel mundial. Los objetivos del proyecto requerían una estructura de alta disponibilidad y un sistema de base de datos replicada basado en MongoDB, implementado a través de un clúster de servidores con cada fragmento replicado.

\cvsubsubsubsection{Tecnologías}
Java, Spring Security, Spring MVC, Spring Data, Spring Documentation, JPA, Hibernate, Wildfly/JBoss, MongoDB, RabbitMQ, JUnit, JSF, Primefaces, GWT, AWS, Phonegap, Hadoop, Selenium, Maven, GIT, Jenkins, Redmine, IntelliJ IDEA, Docker, Ansible

\cvsubsubsection{Gestión Inteligente de Terminales de Iberdrola con Siemens}
El segundo proyecto fue un proyecto conjunto con Siemens para crear la infraestructura de gestión inteligente de terminales de Iberdrola. Estos terminales tomaban información sobre el consumo eléctrico del usuario enviando un informe diario al servidor utilizando el protocolo de comunicación SNMP, a menos que se deseara un monitoreo más detallado de un dispositivo o conjunto de dispositivos específico, en cuyo caso se generarían informes con un nivel de detalle que llegaría al consumo por minuto. Estos datos se almacenaban en una base de datos no relacional MongoDB, que por razones obvias eran varios clústeres, con balanceo de carga y fragmentos replicados. La información de cada punto se almacenaba en el clúster por proximidad geográfica, siempre que el servidor pudiera asumir el volumen de datos que incluía la inclusión de dicho nodo. La información de cada nodo se mantenía durante un año con un nivel de detalle de consumo por minuto, después de un año se compactaba generando el máximo, mínimo y promedio de cada día y esta información se mantenía a este nivel de detalle durante 5 años y pasado ese lapso de tiempo la información de cada nodo se compactaba nuevamente. El sistema también daba la posibilidad de controlar y escuchar en tiempo real el dispositivo a través del protocolo SNMP y JNLP. Esta estructura se creó utilizando microservicios en C++ con el framework de llamada a procedimiento remoto gRPC y se implementó en un conjunto de servidores basados en Red Hat Enterprise Linux.

\cvsubsubsubsection{Tecnologías}
C++, Google Test, Valgrind, cLion, Boost, gRPC, Microservices, Java, RabbitMQ, JPA, Hibernate, Apache Tomcat, MongoDB, JUnit, JSF, Primefaces, Hadoop, Maven, GIT, Jenkins, Jira, IntelliJ IDEA, Docker, Ansible, Kubernetes, JNLP, SNMP, Spring Security, Spring MVC, Spring Data, Spring Documentation, Spring IOT

\cvsubsubsection{Sistema de monitoreo y cuidado para personas con discapacidades o ancianos para Securitas Direct}
El tercer proyecto, ejecutado para Securitas Direct, implicó la creación de un sistema integral de monitoreo y cuidado para personas con discapacidades o ancianos. El sistema identificaba el estado del usuario basándose en diversos sensores distribuidos por su hogar, incluidos sensores de presencia, sensores de apertura de puertas y sensores personales de caídas. Estos datos se recopilaban para proporcionar información detallada sobre las actividades del usuario, que no solo se almacenaba y estaba accesible para consulta, sino que también era capaz de generar alarmas basadas en los patrones típicos de comportamiento del individuo. El sistema analizaba estos patrones y alertaba si se detectaban cambios repentinos, con configuraciones de alarma personalizables.

Además, la aplicación incluía un componente móvil desarrollado como una aplicación híbrida. Esta versión móvil utilizaba sensores para identificar caídas, rastrear la posición GPS del individuo y más.

\cvsubsubsubsection{Tecnologías}
C++, Google Test, Valgrind, cLion, Boost, gRPC, Microservices, Java, AWS, Spring Security, Spring MVC, Spring Data, Spring Documentation, Spring IoC, Spring Batch, RabbitMQ, JPA, Hibernate, Apache Tomcat, Oracle DB, JUnit, Ionic, Typescript, Hadoop, Selenium, Jasmine, Maven, GIT, Jenkins, Jira, Confluence, IntelliJ IDEA, Docker, Ansible, Kubernetes

\cvsubsection{Ingeniero Software Senior}{Grupo Prisa}{Marzo 2012 - Mayo 2015}
Tengo una amplia experiencia trabajando como programador senior remoto en tres proyectos importantes. En este rol, se me confió una variedad de responsabilidades críticas que requerían tanto experiencia técnica como fuertes habilidades de comunicación. Uno de mis deberes principales fue asistir a reuniones con el propietario del producto para definir los requisitos detallados del proyecto. Esto implicaba un análisis exhaustivo de los objetivos del proyecto y desglosarlos en tareas específicas y procesables. Al hacerlo, aseguré que todos los aspectos del proyecto fueran claramente entendidos y abordados desde el principio.

La coordinación con equipos externos fue otro aspecto importante de mi rol. Trabajé en estrecha colaboración con equipos de colaboración con sede en América del Sur, India y China. Esta coordinación internacional requería que gestionara eficazmente diferentes zonas horarias y diferencias culturales. Para facilitar una comunicación y colaboración fluidas, traduje e interpreté los requisitos en chino. Esta habilidad lingüística fue crucial para superar las brechas de comunicación y asegurar que todos los miembros del equipo, independientemente de su ubicación, tuvieran una comprensión clara y precisa de los requisitos del proyecto.

La innovación y la eficiencia fueron áreas clave de enfoque en mi rol. Participé en sesiones de lluvia de ideas para desarrollar e implementar nuevas herramientas destinadas a mejorar la automatización y la productividad del equipo. Estas herramientas estaban diseñadas para agilizar los flujos de trabajo, reducir el esfuerzo manual y mejorar la eficiencia general del proceso de desarrollo. Al introducir estas innovaciones, pude contribuir a un entorno de equipo más productivo y efectivo.

La planificación detallada del proyecto fue otra responsabilidad crítica. Utilicé la metodología SCRUM para asegurar que todas las tareas estuvieran organizadas y gestionadas eficazmente a lo largo del ciclo de vida del proyecto. Esto implicaba crear planes detallados, establecer cronogramas realistas y monitorear continuamente el progreso para asegurar que se cumplieran los hitos del proyecto. El uso de SCRUM permitió un enfoque flexible e iterativo para la gestión de proyectos, permitiendo que el equipo se adaptara a los cambios y abordara los desafíos de manera rápida.

En general, mi experiencia como programador senior remoto me ha dotado de un conjunto de habilidades diverso y una profunda comprensión de la gestión efectiva de proyectos en un entorno de trabajo remoto y global. Mi capacidad para definir requisitos detallados, coordinar con equipos internacionales, innovar para mejorar la productividad y gestionar proyectos utilizando SCRUM me ha permitido contribuir significativamente al éxito de los proyectos en los que he estado involucrado.

\cvsubsubsection{Aplicación de visor de paquetes educativos}
El primer proyecto implicó el desarrollo de una aplicación integral para plataformas Android e iOS destinada a la visualización de paquetes educativos. Esta aplicación fue meticulosamente diseñada para ser compatible con dispositivos de gama baja, asegurando que pudiera alcanzar una base de usuarios más amplia sin comprometer el rendimiento. Era esencial que la aplicación interactuara fluidamente con los usuarios, proporcionando una experiencia sin interrupciones y atractiva. Además, la aplicación estaba equipada con funcionalidad para rastrear y enviar el progreso de los usuarios al servidor, facilitando un monitoreo y evaluación efectivos de su viaje de aprendizaje.

Una característica clave de la aplicación fue su capacidad para manejar paquetes educativos que cumplieran con el estándar SCORM (Modelo de Referencia de Objetos de Contenido Compartible), que se utiliza ampliamente en el campo del e-learning para crear y entregar contenido de aprendizaje interactivo y adaptativo. Para asegurar la compatibilidad con versiones anteriores, la aplicación también fue diseñada para soportar versiones antiguas de paquetes educativos. Esta doble compatibilidad aseguraba que los usuarios pudieran acceder a una amplia gama de contenido educativo, tanto moderno como antiguo, mejorando así la utilidad y el atractivo de la aplicación en varios contextos educativos.

\cvsubsubsubsection{Tecnologías}
Java, Android Studio, XCode, Objective-C, Dagger, SCORM, HTML, JavaScript, Bootstrap, jQuery, Jira, GIT, Confluence

\cvsubsubsection{Proyecto de estandarización de paquetes educativos}
El segundo proyecto surgió de la necesidad de estandarizar los paquetes educativos. En un esfuerzo por ahorrar costos asociados con la migración de Flash a HTML5 bajo el estándar SCORM, se tomó la decisión de desarrollar una herramienta de traducción de Flash a HTML5. Esta herramienta facilitó la conversión de contenido educativo de Flash a HTML5, asegurando la compatibilidad con el estándar SCORM. Al mantener una biblioteca centralizada, la herramienta permitió que cualquier actualización al sistema central se aplicara de manera fluida a todos los paquetes creados anteriormente. Este enfoque agilizó significativamente el proceso de actualización, asegurando consistencia y reduciendo la necesidad de ajustes manuales repetitivos.

Antes de aceptar una nueva versión de la biblioteca centralizada, se realizaron pruebas exhaustivas en un entorno virtualizado. Utilizando tecnologías como QEmu, KVM y Xen, las nuevas versiones fueron evaluadas rigurosamente para asegurar su estabilidad y funcionalidad. Este proceso de prueba integral fue crucial para identificar y resolver posibles problemas antes del despliegue, manteniendo así la integridad de los paquetes educativos. El proyecto no solo logró ahorros de costos y mejoró la eficiencia, sino que también proporcionó un marco robusto para futuras actualizaciones y mejoras, asegurando la longevidad y adaptabilidad del contenido educativo.

\cvsubsubsubsection{Tecnologías}
C++, Linux, Flash, HTML, JavaScript, AngularJS, Bootstrap, SCORM, jQuery, Jira, GIT, Confluence, QEmu, KVM, Xen, Bash, NginX, Valgrind

\cvsubsubsection{Proyecto de plataforma educativa}
El tercer proyecto se desarrolló utilizando C\# en la plataforma .NET, diseñado como un entorno integral para profesores y profesionales de la educación. Esta plataforma permitía a los usuarios gestionar diversas tareas educativas, incluyendo la gestión de estudiantes, el manejo de paquetes educativos y el análisis de sus resultados. También facilitaba la programación y ejecución de tareas relacionadas con el servidor de medios para fines de transmisión.

Además, la plataforma contaba con un centro de comunicación de audio y video de alta resolución vía IP, mejorando las interacciones en tiempo real entre educadores y estudiantes. Este sistema robusto soportaba un flujo de trabajo sin interrupciones y eficiente, asegurando que todas las actividades educativas se gestionaran y ejecutaran efectivamente dentro de un entorno unificado.

\cvsubsubsubsection{Tecnologías}
C\#, .NET, IIS, Windows Server, Microsoft SQL Server, Jira, GIT, Confluence

\cvsubsection{Ingeniero Software Senior}{Intelygenz}{Enero 2010 - Enero 2012}
Como Programador Senior, contribuí a tres proyectos significativos. El primer proyecto fue una aplicación para Antena 3 Televisión, desarrollada de forma nativa para Android. Esta aplicación servía como una interfaz interactiva en tiempo real con el contenido televisivo, actualizada a través de marcadores inaudibles incrustados en el audio. El audio era interceptado por la aplicación, utilizando una solución personalizada desarrollada en el NDK de Android para asegurar un alto rendimiento y bajo consumo de batería. Esta solución modificaba el audio añadiendo ecos separados por distancias temporales específicas, que se interpretaban de manera binaria.

\cvsubsubsection{Proyecto de Antena 3 Televisión}
El primer proyecto involucró el desarrollo de una aplicación para Antena 3 Televisión, construida de forma nativa para Android. Esta aplicación servía como una interfaz interactiva en tiempo real con el contenido televisivo, actualizada dinámicamente utilizando marcadores inaudibles incrustados en el audio. Para lograr esto, la aplicación utilizaba una solución personalizada desarrollada en el NDK de Android, que aseguraba un mayor rendimiento y menor consumo de batería.

El audio modificado incluía ecos separados por intervalos temporales específicos, que la aplicación interpretaba en formato binario. Este enfoque innovador permitía que la aplicación interactuara sin problemas con la transmisión televisiva, proporcionando a los usuarios una experiencia de visualización mejorada y atractiva.

\cvsubsubsubsection{Tecnologías}
Java, Android Studio, Android NDK, Jira, GIT, Confluence

\cvsubsubsection{Proyecto de gestor de documentos para BBVA}
El segundo proyecto involucró el desarrollo de un gestor de documentos para el banco BBVA, implementado en C\# dentro del entorno .NET. Este sistema fue diseñado para conectarse con herramientas externas basadas en el framework Google App Engine. Especializado en documentos financieros, el gestor de documentos facilitaba la creación de grupos de edición, supervisión y acreditación, asegurando una gestión completa de varias tareas relacionadas con los documentos.

Además, el gestor de documentos integraba el flujo de trabajo y la lógica de negocio de BBVA, permitiendo una interacción fluida con otras entidades bancarias. Esta implementación no solo agilizó los procesos de manejo de documentos, sino que también aseguró que las operaciones del banco estuvieran alineadas con los estándares y mejores prácticas de la industria, mejorando así la eficiencia y colaboración entre diferentes departamentos y socios externos.

\cvsubsubsubsection{Tecnologías}
C\#, .NET, IIS, Windows Server, Microsoft SQL Server, Cassandra, HTML, JavaScript, Bootstrap, jQuery, Jira, GIT, Confluence

\cvsubsubsection{Proyecto de Front colaborativo}
El tercer proyecto involucró el desarrollo de un front-end colaborativo para el banco BBVA, utilizando tanto Java como JavaScript. Este proyecto se integró con el entorno NACAR, una plataforma diseñada para mejorar la funcionalidad y experiencia del usuario de aplicaciones bancarias. La integración con NACAR permitió el desarrollo de características avanzadas y una interacción fluida dentro del sistema bancario.

Un logro significativo de este proyecto fue la creación de una biblioteca de gestos y acciones en pantalla, conocida como IRIS. Esta biblioteca fue diseñada para mejorar la interacción del usuario proporcionando formas intuitivas y eficientes de realizar varias tareas. La biblioteca IRIS mejoró la experiencia general del usuario, haciendo que la aplicación fuera más receptiva y fácil de usar.

\cvsubsubsubsection{Tecnologías}
Java, Spring Security, Spring MVC, Spring Data, Spring Documentation, JPA, Hibernate, Websphere, Oracle DB, JUnit, JSF, Maven, GIT, Jira, Confluence, GIT, Eclipse

\cvsubsection{Investigador y personal docente}{UAH}{Enero 2006 - Enero 2009}
Mi rol como investigador en la universidad fue dentro del grupo de investigación TIFyC, donde tuve una gama de responsabilidades enfocadas en avanzar en el campo de la tecnología y la inteligencia artificial. Una de mis principales tareas fue realizar investigaciones exhaustivas sobre el estado del arte en diversas áreas, incluyendo problemas de accesibilidad, algoritmos de IA, visión artificial y sistemas biométricos. Esto implicaba mantenerse actualizado con los últimos avances, evaluar nuevas metodologías e identificar posibles áreas de innovación.

Además de mis deberes de investigación, participé activamente en colaboraciones con grupos de investigación de diferentes universidades. Estas colaboraciones tenían como objetivo fomentar el intercambio de conocimientos, promover la investigación interdisciplinaria e impulsar proyectos innovadores. Al trabajar estrechamente con otros investigadores, pudimos combinar nuestra experiencia y recursos para abordar desafíos complejos y expandir los límites de nuestros respectivos campos.

Otro aspecto clave de mi rol fue la preparación de informes detallados sobre nuestros hallazgos de investigación. Estos informes documentaban nuestras metodologías, resultados y conclusiones, proporcionando un recurso valioso para investigaciones futuras y contribuyendo a la comunidad científica en general. Mi capacidad para comunicar claramente y de manera efectiva nuestros hallazgos fue crucial para asegurar que nuestro trabajo tuviera un impacto significativo.

Además, participé en el estudio y la solicitud de ayudas estatales y subvenciones para proyectos de investigación y desarrollo (I+D). Esto requería una comprensión exhaustiva del panorama de financiación y la capacidad de elaborar propuestas convincentes que demostraran la importancia y el impacto potencial de nuestra investigación. Obtener estos fondos fue esencial para apoyar nuestros proyectos en curso y permitir nuevas iniciativas.

Como parte de mis responsabilidades, también serví como profesor asistente, enseñando cursos sobre Inteligencia Artificial y Sistemas de Visión Artificial. En esta capacidad, fui responsable de impartir conocimientos a los estudiantes, diseñar materiales del curso y guiarlos a través de conceptos complejos y aplicaciones prácticas. Mi rol de enseñanza me permitió compartir mi pasión por la IA y la visión artificial con la próxima generación de investigadores y profesionales, al mismo tiempo que me mantenía comprometido con la comunidad académica.

\cvsubsubsection{Proyecto del grupo de investigación}
Uno de los proyectos significativos llevados a cabo por nuestro grupo de investigación se centró en investigar dispositivos innovadores diseñados para mejorar la interacción para individuos con discapacidades visuales, auditivas o motoras. El objetivo principal de esta investigación era desarrollar y perfeccionar tecnologías que pudieran facilitar interacciones más intuitivas y accesibles con varios tipos de sistemas, incluidos computadoras de escritorio, portátiles y dispositivos móviles.

Nuestro equipo exploró una amplia gama de tecnologías de asistencia y mejoras de interfaces de usuario destinadas a superar los desafíos que enfrentan las personas con discapacidades. Esto incluyó el desarrollo de dispositivos de entrada especializados, soluciones de software adaptativas y diseños avanzados de interfaces de usuario que se adaptaran específicamente a las necesidades de los usuarios con discapacidades visuales, auditivas y motoras. Al enfocarnos en estas áreas, buscábamos crear un entorno digital más inclusivo que empoderara a los usuarios para interactuar con la tecnología de manera más efectiva e independiente.

La investigación también involucró extensas pruebas de usuario y colaboración con individuos que tienen discapacidades para asegurar que las soluciones desarrolladas fueran prácticas, fáciles de usar y realmente beneficiosas. Al integrar comentarios de usuarios reales, nuestro grupo de investigación pudo hacer avances significativos en la creación de dispositivos y aplicaciones que no solo cumplieran con los estándares de accesibilidad, sino que también mejoraran la experiencia general del usuario para personas con discapacidades.

Este proyecto ejemplifica nuestro compromiso de aprovechar la tecnología para eliminar barreras y mejorar la calidad de vida de las personas con discapacidades, haciendo que la tecnología sea accesible y usable para todos, independientemente de sus capacidades físicas.

\cvsubsubsubsection{Tecnologías}
Java, Android Studio, Android, Linux, C++, Anjuta, Qt, PIC, Arduino, SVN, C\#, .NET, Mono

\cvsubsubsection{Sistema de comunicación para dispositivos biométricos}
El segundo proyecto en el que trabajé implicó desarrollar un sistema de comunicación estandarizado para dispositivos biométricos. El objetivo era habilitar un intercambio de mensajes y colaboración sin problemas entre dispositivos de varios fabricantes. Para lograr esto, integramos un sistema de llamadas a procedimientos remotos (RPC) dentro de los propios dispositivos. Este sistema RPC aseguraba que la comunicación entre los dispositivos permaneciera transparente, independientemente del lenguaje de programación o sistema operativo utilizado.

El proyecto requería una consideración cuidadosa de los problemas de interoperabilidad, ya que los dispositivos biométricos a menudo vienen con protocolos de comunicación propietarios y especificaciones técnicas diversas. Al implementar un sistema RPC estandarizado, pudimos abstraer las diferencias subyacentes y crear un protocolo de comunicación unificado. Este protocolo facilitaba un intercambio eficiente de datos y una coordinación operativa en una amplia gama de dispositivos biométricos.

Además, el sistema de comunicación fue diseñado para ser altamente robusto y seguro, dado el carácter sensible de los datos biométricos. Incorporamos mecanismos de encriptación y autenticación para proteger la integridad de los datos y prevenir el acceso no autorizado. El resultado final fue una solución confiable y multiplataforma que mejoró considerablemente la interoperabilidad y funcionalidad de los sistemas biométricos.

Este proyecto no solo mejoró la compatibilidad de los dispositivos, sino que también allanó el camino para futuros avances en tecnología biométrica, permitiendo a los fabricantes centrarse en la innovación sin preocuparse por las barreras de comunicación. El sistema de comunicación estandarizado representó así un avance significativo en el campo de la integración de dispositivos biométricos.

\cvsubsubsubsection{Tecnologías}
Java, Linux, C++, Anjuta, SVN, C\#, .NET, Mono

\cvsubsubsection{Proyecto de página web del grupo de investigación}
El tercer proyecto involucró la creación y el mantenimiento continuo de la página web del grupo de investigación utilizando el Sistema de Gestión de Contenidos (CMS) Joomla. Este proyecto requería no solo el desarrollo inicial de una página web fácil de usar y visualmente atractiva, sino también la actualización y mejora continuas de sus características y contenido para satisfacer las necesidades en evolución del grupo de investigación.

El sitio web servía como un centro central para las actividades del grupo de investigación, proporcionando acceso a publicaciones, actualizaciones de proyectos y eventos futuros. Aprovechando el robusto framework de Joomla, aseguré que el sitio web fuera tanto seguro como escalable, capaz de manejar un volumen creciente de contenido e interacciones de usuarios. El CMS permitía una gestión de contenido fácil, habilitando a los miembros del grupo de investigación a contribuir y actualizar información sin requerir conocimientos técnicos extensos.

Además de la implementación técnica, me enfoqué en optimizar el rendimiento y la accesibilidad del sitio web. Esto incluía asegurar tiempos de carga rápidos, implementar un diseño responsivo para la compatibilidad con varios dispositivos y adherirse a los estándares de accesibilidad para hacer que el sitio fuera usable para todos los visitantes. Las tareas de mantenimiento regular incluían actualizar el núcleo de Joomla y las extensiones, monitorear el rendimiento del sitio y abordar cualquier vulnerabilidad de seguridad de manera oportuna.

En resumen, este proyecto demostró mi capacidad para desarrollar y gestionar una presencia web dinámica que respalda efectivamente los objetivos y actividades de un grupo especializado, utilizando las potentes características del CMS Joomla.

\cvsubsubsubsection{Tecnologías}
Java, Linux, C++, Anjuta, SVN, C\#, .NET, Mono

\cvsubsubsection{Contribuciones al grupo de investigación}
Durante mi permanencia en el grupo de investigación, contribuí en diversas capacidades. Mi trabajo de apoyo abarcó una variedad de tareas destinadas a facilitar los objetivos del grupo. También organicé y conduje seminarios especializados enfocados en la Robótica, compartiendo conocimientos avanzados y los últimos desarrollos en el campo con mis colegas y estudiantes.

Además, serví como profesor asistente, donde enseñé asignaturas como Inteligencia Artificial y Visión Artificial. En este rol, fui responsable de preparar y dictar conferencias, asistir en el diseño del curso y brindar orientación a los estudiantes sobre temas complejos relacionados con las tecnologías de IA y visión por computadora. Mi participación en estas asignaturas ayudó a cerrar la brecha entre los conceptos teóricos y las aplicaciones prácticas para los estudiantes.

Más allá de los roles de enseñanza y apoyo, participé activamente en actividades extracurriculares, incluyendo el concurso de robótica Hispabot. Competí en el modo de resolución de laberintos, donde apliqué mi experiencia en robótica e IA para diseñar y programar robots capaces de navegar y resolver laberintos. Esta experiencia no solo mejoró mis habilidades técnicas, sino que también demostró mi capacidad para aplicar la investigación en entornos competitivos del mundo real.

\cvsubsubsubsection{Tecnologías}
C, C++, Arduino, PIC, Mathlab, Octave, Java, Python

\cvsubsection{Programador junior}{Knowcentury}{Enero 2005 - Julio 2005}
Durante mis primeros años en la universidad, con el fin de poder pagar mis estudios, trabajé como programador web junior utilizando CMS en Knowcentury.

\cvsubsubsection{Desarrollo de sitios web}
Implementación de páginas web utilizando CMS como Joomla, Drupal y Moodle.

\cvsubsubsubsection{Tecnologías}
Joomla, PHP, JavaScript, MySQL, HTML

\cvsubsection{Programador junior}{IBM}{Septiembre 2003 - Diciembre 2004}
Durante este período, mi responsabilidad principal fue realizar una investigación exhaustiva sobre el uso del mainframe IBM System Z/390s para la virtualización de sistemas. Este proyecto implicaba utilizar Qemu, un emulador de código abierto, para facilitar el proceso de virtualización. El objetivo era habilitar el uso de sistemas remotos creando entornos virtualizados en el mainframe de IBM. Estos sistemas virtuales debían operar bajo el sistema operativo Suse Linux Enterprise Edition, asegurando una plataforma robusta y segura para diversas aplicaciones y servicios.

Mi rol requería una comprensión integral de la arquitectura y capacidades del IBM System Z/390s. Me centré en explorar cómo Qemu podía integrarse eficazmente con el mainframe para lograr una virtualización eficiente. Esto implicaba configurar y optimizar el mainframe para soportar múltiples máquinas virtuales, cada una ejecutando su instancia del Suse Linux Enterprise Edition. El uso de Suse Linux era crítico ya que proporcionaba la estabilidad necesaria y las características de nivel empresarial requeridas para sistemas virtuales de alto rendimiento y fiabilidad.

Además, investigué varios aspectos de la virtualización de sistemas, incluyendo la asignación de recursos, la optimización del rendimiento y las medidas de seguridad. Asegurar que los sistemas virtualizados pudieran operar sin problemas y eficientemente en el mainframe era primordial. Esto implicaba pruebas exhaustivas y ajustes finos de los entornos virtuales para cumplir con las necesidades específicas de los usuarios de sistemas remotos. El proyecto tenía como objetivo aprovechar el poder y la fiabilidad del IBM System Z/390s para proporcionar soluciones virtualizadas escalables y seguras para aplicaciones empresariales.

\cvsubsubsection{Desarrollo de sitios web}
Implementación de páginas web utilizando CMS como Joomla, Drupal y Moodle.

\cvsubsubsubsection{Tecnologías}
Joomla, PHP, JavaScript, MySQL, HTML

\cvsection{Educación}
\cvsubsection{Máster en Inteligencia Artificial en tecnologías de la información y la comunicación}{UAH}{Septiembre 2007 - Junio 2009}
Inteligencia artificial y aprendizaje automático. Supervisado, no supervisado y reforzado. Aprendizaje simbólico. Modelos de clasificación y regresión. Optimización de modelos.

Redes profundas. Redes multicapa, retropropagación. Funciones de pérdida. Hiperparámetros y estrategias de aprendizaje.

Redes convolucionales. Reconocimiento de imágenes.

Redes secuenciales y recurrentes. Modelos LSTM.

Técnicas de paralelización y computación en arquitecturas basadas en GPU.

Técnicas de vectorización. Programación con tensorflow y theano.

Aprendizaje automatizado escalable. Frameworks de paralelización en clústeres de computadoras.

Aplicaciones en medicina, finanzas, conducción automática, etc.

\cvsubsection{Ingeniería Técnica en Informática}{UAH}{Septiembre 2001 - Junio 2007}
Con honores en: Inteligencia Artificial, Lógica Difusa, Sistemas de Visión Artificial, Física II, Análisis Matemático, Estructura de Datos, Redes.

\cvsubsection{LPIC - 2}{Fundación Linux}{Septiembre 2015}
Certificación oficial de administración de sistemas Linux que implica la calificación para:

Administración avanzada de sistemas Linux, incluidos los relacionados con el kernel.

Realizar gestión avanzada del almacenamiento de bloques y sistemas de archivos, así como la autenticación avanzada de redes y la seguridad del sistema, incluidos el firewall y la VPN

Instalar y configurar servicios centrales de red (DHCP, DNS, SSH, servidores web, servidores de archivos, correo electrónico, etc.).

Monitorizar asistentes y asesorar a la administración sobre automatización y compras

\cvsubsection{LPIC - 1}{Fundación Linux}{Septiembre 2013}
Certificación oficial de administración de sistemas Linux que implica la calificación para:

Instalar un sistema Linux, incluido X11 y configurarlo como un cliente de red.

Trabajar en la línea de comandos.

Gestionar archivos y permisos de acceso, incluidos ACL, así como la seguridad del sistema.

Realizar tareas de mantenimiento sencillas.

\cvsubsection{Diseño y evaluación de contenidos educativos digitales}{Universidad de Alcalá de Henares}{Septiembre 2007}
Diseño de contenido educativo a través del estándar SCORM.

\cvsubsection{Curso de visión artificial}{Universidad de Alcalá de Henares}{Septiembre 2007}
Sistemas de visión artificial, matrices de convolución, filtros.

\cvsection{Idiomas}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{ l c c r }
  \textbf{Language} & \textbf{Spoken} & \textbf{Writen} & \textbf{Read} \\
\hline
\textbf{Español} & Nativo & Nativo & Nativo\\
\textbf{Inglés} & Fluido & Fluido & Fluido\\
\textbf{Chino} & Fluido & Fluido & Fluido\\
\end{tabular}
\cvsubsection{Acreditaciones}{}{}
\cvsubsubsection{Chino}
\cvsubsubsubsection{HSK 3 Instituto Confucio Madrid [Septiembre 2009]}
\end{document}