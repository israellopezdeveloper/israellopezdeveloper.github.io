---
title: UMAP vs t-SNE
date: '2026-01-06'
summary: 'What they optimize, when to use them and when not to.'
---

<script lang="ts">
export const meta = {
    title: "{title}",
    date: "{date}",
    summary: "{summary}"
  };
</script>

# {title}

One of the topics people rarely think about when they think about Artificial Intelligence.
Yet it is widely used by companies like Google, Meta, Amazon, or Netflix for tasks such as:

* **Embedding visualization:**

  * embedding validation
  * detection of unexpected embeddings
  * identification of noise, outliers, or bias

> For example, Google uses t-SNE to visualize word and image embeddings in TensorBoard.

* **Bioinformatics and genomics:**

  * cell type identification
  * cellular states
  * validation of biological clusters

* **Cybersecurity and fraud detection:**

  * visualization of transactions and traffic patterns
  * exploratory fraud analysis
  * decision support for human analysts
  * detection of anomalous behavior

* **Model monitoring in production**

In short, t-SNE and UMAP are rarely used directly for production inference,
but they are key tools for analytical production workflows, model validation, monitoring, and decision-making.

## UMAP and t-SNE: what they are really doing when they “draw” your data

This has probably happened to you:

You run **UMAP** or **t-SNE**, you see clear clusters, and you think:

> “Okay, there’s structure here.”

Sometimes there is.
Often, not exactly in the way you think.

Not because the technique is bad — quite the opposite —
but because **we often don’t understand the mathematical problem they are actually solving**.

## The basic intuition (no formulas… yet)

Both **t-SNE** and **UMAP** start from the same simple idea:

> *In the original space, some points are neighbors.
> In 2D, those neighbors should remain neighbors.*

That’s it.

They do **not** try to:

* preserve real distances
* respect sizes
* maintain global proportions

They only try to **avoid breaking neighborhoods**.

Everything else is negotiable.

## t-SNE: a magnifying glass for local structure

*(Distill, 2016)*

At a high level, t-SNE works like this:

1. It looks at your data in high dimensions
2. It converts distances into **neighborhood probabilities**
3. It does the same in 2D
4. It moves points so those probabilities match as closely as possible

Mathematically, it minimizes a **KL divergence** between two probability distributions:

$$
KL(P \mid Q)
$$

And here is the key that explains **most misunderstandings**:

* This divergence heavily penalizes **losing close neighbors**,
* but it is largely indifferent to global distortions.

That’s why, as explained by Distill:

* the distance between clusters means nothing
* cluster size means nothing
* empty space means nothing

t-SNE does not tell you *how different* two groups are.
It only tells you *which points are locally similar*.

It is a **magnifying glass**, not a map.

## UMAP: the same idea, but more structured

*(McInnes et al., 2018 · PAIR)*

UMAP starts from a similar intuition but adds a more “geometric” layer.

UMAP assumes that:

* data lives on some kind of surface (a *manifold*)
* locally, that surface is coherent
* nearby distances matter more than distant ones

Based on this, it builds a **neighbor graph**:
who is connected to whom, and with what strength.

That strength is modeled with probabilities of the form:

$$
p_{ij} = \exp\left(-\frac{d(x_i, x_j) - \rho_i}{\sigma_i}\right)
$$

There’s no need to obsess over the formula.
The idea is simple:

> “The closer two points are, the stronger their connection.”

UMAP then tries to place the points in 2D so that this **graph** looks as similar as possible in both spaces, minimizing a **cross-entropy** between them.

* UMAP does not preserve variance (like PCA).
* It preserves **topological relationships**.

That’s why it often “looks better”.
But it is still an **optimization**, not an absolute truth.

## Why changing one parameter can completely change the plot

This is where both **PAIR** and the 2021 paper fit perfectly.

Parameters like:

* `n_neighbors`
* `min_dist`
* the distance metric

are not technical details.

They are **assumptions about the data**.

Changing `n_neighbors` is changing the question:

> “What do I consider local?”

From a mathematical perspective:

* the problem is **non-convex**
* there are multiple valid solutions
* different initializations → different minima

That’s why the 2021 paper shows that:

> small changes can produce very different visualizations

Not because UMAP is “failing”,
but because **there is no single correct embedding**.

## So… when are they actually useful?

They are very useful when used **for what they are**:

* exploring embeddings
* detecting unusual neighborhoods
* debugging representations
* generating hypotheses

Real examples:

* text embeddings
* image embeddings
* exploratory analysis before modeling

This is where they shine.

## And when should you avoid them?

When they are used to:

* decide how many clusters exist
* justify business decisions
* measure real similarity
* “prove” structure

If your conclusion depends only on the 2D plot,
the problem is not UMAP or t-SNE.

It’s the interpretation.

## A simple rule that almost never fails

* **PCA** → understand variance and linear structure
* **t-SNE** → inspect local relationships
* **UMAP** → explore local structure at scale
* **Metrics + domain knowledge** → decide

If an idea disappears when you:

* change parameters
* change the random seed
* change the sample

it was probably never a solid idea to begin with.

## Closing

All the referenced resources agree on the same point, even if they use different language:

> Dimensionality reduction does not discover structure.
> **It optimizes a notion of structure.**

The difference between junior and senior is not knowing how to run UMAP.
It’s knowing **when the picture helps… and when it lies**.

## References

* McInnes et al., ***UMAP: Uniform Manifold Approximation and Projection*** (2018)
  [https://arxiv.org/abs/1802.03426](https://arxiv.org/abs/1802.03426)

* ***On the instability and misinterpretation of dimensionality reduction visualizations*** (2021)
  [https://arxiv.org/abs/2102.01384](https://arxiv.org/abs/2102.01384)

* Distill.pub — ***How to Use t-SNE Effectively*** (2016)
  [https://distill.pub/2016/misread-tsne/](https://distill.pub/2016/misread-tsne/)

* Google PAIR — ***Understanding UMAP***
  [https://pair-code.github.io/understanding-umap/](https://pair-code.github.io/understanding-umap/)
